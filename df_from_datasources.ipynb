{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import and initiate findspark to begin with.\n",
    "And import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now start SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/19 15:17:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/10/19 15:17:33 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/10/19 15:17:33 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "22/10/19 15:17:33 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "22/10/19 15:17:33 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "22/10/19 15:17:33 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "22/10/19 15:17:33 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.\n",
      "22/10/19 15:17:33 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.\n",
      "22/10/19 15:17:33 WARN Utils: Service 'SparkUI' could not bind on port 4047. Attempting port 4048.\n",
      "22/10/19 15:17:33 WARN Utils: Service 'SparkUI' could not bind on port 4048. Attempting port 4049.\n",
      "22/10/19 15:17:33 WARN Utils: Service 'SparkUI' could not bind on port 4049. Attempting port 4050.\n",
      "22/10/19 15:17:33 WARN Utils: Service 'SparkUI' could not bind on port 4050. Attempting port 4051.\n",
      "22/10/19 15:17:33 WARN Utils: Service 'SparkUI' could not bind on port 4051. Attempting port 4052.\n",
      "22/10/19 15:17:33 WARN Utils: Service 'SparkUI' could not bind on port 4052. Attempting port 4053.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Python Spark SQL example\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create DataFrame from data source - csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "customerDF = spark.read.load(\"customers.txt\", format=\"csv\", sep=\"\\t\", inferSchema=\"true\", header=\"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different types of operations on DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- customer_name: string (nullable = true)\n",
      " |-- customer_city: string (nullable = true)\n",
      " |-- customer_state: string (nullable = true)\n",
      " |-- customer_zipcode: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customerDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|   customer_name|\n",
      "+----------------+\n",
      "|     Mary Torres|\n",
      "|      Jose Haley|\n",
      "|      Mary Smith|\n",
      "|  Richard Maddox|\n",
      "|  Margaret Booth|\n",
      "|  Mary Henderson|\n",
      "|     Lisa Walker|\n",
      "|   Jonathan Hill|\n",
      "|Carolyn Sheppard|\n",
      "|    Mary Mendoza|\n",
      "|   Michael Smith|\n",
      "|    James Holmes|\n",
      "|     Mary Dawson|\n",
      "|    Adam Marquez|\n",
      "|    Gloria Smith|\n",
      "|       Mary Webb|\n",
      "|  Nancy Alvarado|\n",
      "|  Russell Flores|\n",
      "|    Denise Smith|\n",
      "|  Jose Dickerson|\n",
      "|      Mary Smith|\n",
      "|   Michelle Rose|\n",
      "|Russell Peterson|\n",
      "|      Mary Smith|\n",
      "| Mary Pennington|\n",
      "|      Mary Smith|\n",
      "|    Jean Donovan|\n",
      "|     Louis Novak|\n",
      "|     Mary Santos|\n",
      "|    Mary Harding|\n",
      "+----------------+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customerDF.select(\"customer_name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+\n",
      "|   customer_name|customer_city|\n",
      "+----------------+-------------+\n",
      "|     Mary Torres|       Caguas|\n",
      "|      Jose Haley|     Columbus|\n",
      "|      Mary Smith|      Houston|\n",
      "|  Richard Maddox|       Caguas|\n",
      "|  Margaret Booth|    Arlington|\n",
      "|  Mary Henderson|       Caguas|\n",
      "|     Lisa Walker|       Caguas|\n",
      "|   Jonathan Hill|      Phoenix|\n",
      "|Carolyn Sheppard|Pompano Beach|\n",
      "|    Mary Mendoza|       Caguas|\n",
      "|   Michael Smith|       Caguas|\n",
      "|    James Holmes|     Hilliard|\n",
      "|     Mary Dawson|       Caguas|\n",
      "|    Adam Marquez|  San Antonio|\n",
      "|    Gloria Smith|       Caguas|\n",
      "|       Mary Webb|   San Marcos|\n",
      "|  Nancy Alvarado|     Flushing|\n",
      "|  Russell Flores|       Caguas|\n",
      "|    Denise Smith|    Rego Park|\n",
      "|  Jose Dickerson|         Mesa|\n",
      "+----------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customerDF.select(customerDF['customer_name'], customerDF['customer_city']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+---------------+--------------+----------------+\n",
      "|customer_id|   customer_name|  customer_city|customer_state|customer_zipcode|\n",
      "+-----------+----------------+---------------+--------------+----------------+\n",
      "|       5577|      Mary Smith|        Modesto|            CA|           95350|\n",
      "|       1745|      Mary Smith|Rowland Heights|            CA|           91748|\n",
      "|      11444|Kathleen Patrick|      San Diego|            CA|           92109|\n",
      "|       8846|    Thomas Smith|          Indio|            CA|           92201|\n",
      "|       6237|  Bobby Anderson|       El Cajon|            CA|           92020|\n",
      "+-----------+----------------+---------------+--------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customerDF.filter(customerDF['customer_state'] == 'CA').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|customer_state|count|\n",
      "+--------------+-----+\n",
      "|            AZ|   19|\n",
      "|            SC|    2|\n",
      "|            LA|    7|\n",
      "|            MN|    1|\n",
      "|            NJ|   19|\n",
      "|            DC|    4|\n",
      "|            OR|    4|\n",
      "|            VA|   14|\n",
      "|            RI|    2|\n",
      "|            KY|    1|\n",
      "|            MI|   28|\n",
      "|            NV|   16|\n",
      "|            WI|    9|\n",
      "|            ID|    2|\n",
      "|            CA|  187|\n",
      "|            CT|    8|\n",
      "|            NC|   19|\n",
      "|            MD|   19|\n",
      "|            DE|    1|\n",
      "|            MO|   13|\n",
      "+--------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customerDF.groupBy(\"customer_state\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create temp view for running SQL queries on the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "customerDF.createOrReplaceTempView(\"customers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SQL can be run on DataFrames that are registered as temp views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cStateCount50 = spark.sql(\"SELECT customer_state, count(*) as state_count FROM customers GROUP BY \\\n",
    "customer_state HAVING state_count>=50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cStateCount50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+\n",
      "|customer_state|state_count|\n",
      "+--------------+-----------+\n",
      "|            CA|        187|\n",
      "|            NY|         79|\n",
      "|            TX|         62|\n",
      "|            PR|        505|\n",
      "+--------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cStateCount50.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_state: string (nullable = true)\n",
      " |-- state_count: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cStateCount50.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "cStateCount50.coalesce(1).write.parquet(\"cStateOutput1.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the dataframe is small we can coalesce all the partitions into one and write it. This will result in a single file output else output will be in as many files as the number of partitions. coalesce can prove expensive on large dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively the following syntax is also allowed.\n",
    "We can use the above since parquet is the default format used by SparkSQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cStateCount50.coalesce(1).write.save(\"cStateOutput2.parquet\", format=\"parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cStateCount50.coalesce(1).write.save(\"cStateOutput3.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cStateCount50.coalesce(1).write.json(\"cStateOutput1.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively the following syntax is also allowed.\n",
    "\n",
    "cStateCount50.coalesce(1).write.save(\"cStateOutput2.json\", format=\"json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Create DataFrame from data source - JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "productDF = spark.read.load(\"products.json\", format=\"json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively the following syntax is also allowed\n",
    "\n",
    "productDF = spark.read.json(\"products.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run set of the DataFrame operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- category_id: long (nullable = true)\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- product_category: string (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- product_price: double (nullable = true)\n",
      " |-- product_quantity: long (nullable = true)\n",
      " |-- salestxn_id: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "productDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|        product_name|\n",
      "+--------------------+\n",
      "|O'Brien Men's Neo...|\n",
      "|O'Brien Men's Neo...|\n",
      "|Under Armour Wome...|\n",
      "|O'Brien Men's Neo...|\n",
      "|Pelican Sunstream...|\n",
      "|Nike Men's CJ Eli...|\n",
      "|Diamondback Women...|\n",
      "|Field & Stream Sp...|\n",
      "|Perfect Fitness P...|\n",
      "|Nike Men's CJ Eli...|\n",
      "|Pelican Sunstream...|\n",
      "|Nike Men's CJ Eli...|\n",
      "|Diamondback Women...|\n",
      "|Nike Men's CJ Eli...|\n",
      "|Nike Men's Dri-FI...|\n",
      "|O'Brien Men's Neo...|\n",
      "|O'Brien Men's Neo...|\n",
      "|Nike Men's Dri-FI...|\n",
      "|Diamondback Women...|\n",
      "|Under Armour Girl...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "productDF.select(\"product_name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+-------------+\n",
      "|        product_name|  product_category|product_price|\n",
      "+--------------------+------------------+-------------+\n",
      "|O'Brien Men's Neo...|           Fishing|        49.98|\n",
      "|O'Brien Men's Neo...|           Fishing|        49.98|\n",
      "|Under Armour Wome...|      Boxing & MMA|        31.99|\n",
      "|O'Brien Men's Neo...|           Fishing|        49.98|\n",
      "|Pelican Sunstream...|           Boating|       199.99|\n",
      "|Nike Men's CJ Eli...|            Cleats|       129.99|\n",
      "|Diamondback Women...| Bike & Skate Shop|       299.98|\n",
      "|Field & Stream Sp...|Hunting & Shooting|       399.98|\n",
      "|Perfect Fitness P...|   As Seen on  TV!|        59.99|\n",
      "|Nike Men's CJ Eli...|            Cleats|       129.99|\n",
      "|Pelican Sunstream...|           Boating|       199.99|\n",
      "|Nike Men's CJ Eli...|            Cleats|       129.99|\n",
      "|Diamondback Women...| Bike & Skate Shop|       299.98|\n",
      "|Nike Men's CJ Eli...|            Cleats|       129.99|\n",
      "|Nike Men's Dri-FI...|     Men's Apparel|         50.0|\n",
      "|O'Brien Men's Neo...|           Fishing|        49.98|\n",
      "|O'Brien Men's Neo...|           Fishing|        49.98|\n",
      "|Nike Men's Dri-FI...|     Men's Apparel|         50.0|\n",
      "|Diamondback Women...| Bike & Skate Shop|       299.98|\n",
      "|Under Armour Girl...|        Top Brands|        39.99|\n",
      "+--------------------+------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "productDF.select(productDF['product_name'], productDF['product_category'], productDF['product_price']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+------------------+--------------------+-------------+----------------+-----------+\n",
      "|category_id|customer_id|  product_category|        product_name|product_price|product_quantity|salestxn_id|\n",
      "+-----------+-----------+------------------+--------------------+-------------+----------------+-----------+\n",
      "|         42|        702| Bike & Skate Shop|Diamondback Women...|       299.98|               1|     140220|\n",
      "|         44|       3959|Hunting & Shooting|Field & Stream Sp...|       399.98|               1|      77426|\n",
      "|         42|       5658| Bike & Skate Shop|Diamondback Women...|       299.98|               1|      84894|\n",
      "|         42|       9356| Bike & Skate Shop|Diamondback Women...|       299.98|               1|     102807|\n",
      "|         42|       8651| Bike & Skate Shop|Diamondback Women...|       299.98|               1|     134324|\n",
      "|         44|      12072|Hunting & Shooting|Field & Stream Sp...|       399.98|               1|      27476|\n",
      "|         42|       7830| Bike & Skate Shop|Diamondback Women...|       299.98|               1|     154460|\n",
      "|         42|       6995| Bike & Skate Shop|Diamondback Women...|       299.98|               1|     109562|\n",
      "|         42|      12428| Bike & Skate Shop|Diamondback Women...|       299.98|               1|     172090|\n",
      "|         44|       7013|Hunting & Shooting|Field & Stream Sp...|       399.98|               1|       3516|\n",
      "|         44|       9992|Hunting & Shooting|Field & Stream Sp...|       399.98|               1|     150088|\n",
      "|         42|       7816| Bike & Skate Shop|Diamondback Women...|       299.98|               1|     164863|\n",
      "|         44|      10300|Hunting & Shooting|Field & Stream Sp...|       399.98|               1|      32507|\n",
      "|         44|       2152|Hunting & Shooting|Field & Stream Sp...|       399.98|               1|      50567|\n",
      "|         44|        602|Hunting & Shooting|Field & Stream Sp...|       399.98|               1|      79804|\n",
      "|         44|        685|Hunting & Shooting|Field & Stream Sp...|       399.98|               1|      68394|\n",
      "|         44|       7396|Hunting & Shooting|Field & Stream Sp...|       399.98|               1|      45827|\n",
      "|         42|      11165| Bike & Skate Shop|Diamondback Women...|       299.98|               1|      99392|\n",
      "|         44|      11584|Hunting & Shooting|Field & Stream Sp...|       399.98|               1|      30107|\n",
      "|         44|       6887|Hunting & Shooting|Field & Stream Sp...|       399.98|               1|     108056|\n",
      "+-----------+-----------+------------------+--------------------+-------------+----------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "productDF.filter(productDF['product_price'] > 200.00).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|   product_category|count|\n",
      "+-------------------+-----+\n",
      "|  Training by Sport|    5|\n",
      "|   Men's Golf Clubs|   21|\n",
      "|   Camping & Hiking|   44|\n",
      "|Fitness Accessories|   47|\n",
      "|         Golf Shoes|    6|\n",
      "|         Basketball|   36|\n",
      "|        Electronics|   48|\n",
      "|          Team Shop|  162|\n",
      "|      Men's Apparel| 2085|\n",
      "|  Bike & Skate Shop| 1377|\n",
      "|  Golf Bags & Carts|   89|\n",
      "|    As Seen on  TV!| 2399|\n",
      "|       Boxing & MMA|  115|\n",
      "| Hunting & Shooting| 1785|\n",
      "|Baseball & Softball|    4|\n",
      "|       Golf Apparel|   51|\n",
      "| Women's Golf Clubs|   57|\n",
      "|      Shop By Sport|   26|\n",
      "|            Fishing| 1953|\n",
      "|        Accessories|  110|\n",
      "+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "productDF.groupBy(\"product_category\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create temp view for running SQL queries on the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "productDF.createOrReplaceTempView(\"products\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "SQL queries can be run on the DataFrames that have been registered as a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "prd200 = spark.sql(\"SELECT category_id, product_category, count(*) as prdcount FROM products \\\n",
    "WHERE product_price>200 \\\n",
    "GROUP BY category_id, product_category ORDER BY product_category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(prd200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+--------+\n",
      "|category_id|   product_category|prdcount|\n",
      "+-----------+-------------------+--------+\n",
      "|         40|        Accessories|       7|\n",
      "|         16|    As Seen on  TV!|       6|\n",
      "|          3|Baseball & Softball|       4|\n",
      "|         42|  Bike & Skate Shop|    1377|\n",
      "|         47|            Boating|       6|\n",
      "|          9|   Cardio Equipment|       3|\n",
      "|         37|        Electronics|       9|\n",
      "|         34|  Golf Bags & Carts|      10|\n",
      "|         44| Hunting & Shooting|    1785|\n",
      "|         30|   Men's Golf Clubs|       5|\n",
      "|         10|  Strength Training|       2|\n",
      "|          6|   Tennis & Racquet|       3|\n",
      "+-----------+-------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prd200.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- category_id: long (nullable = true)\n",
      " |-- product_category: string (nullable = true)\n",
      " |-- prdcount: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prd200.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "prd200.coalesce(1).write.save(\"product1.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively the following syntax is also allowed\n",
    "\n",
    "prd200.write.save(\"product2.parquet\", format=\"parquet\")\n",
    "\n",
    "prd200.write.parquet(\"product1.parquet\")\n",
    "\n",
    "Since parquet is the default format used by SparkSQL we need not specify it as shown in the first command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "prd200.coalesce(1).write.json(\"product1.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively the following syntax is also allowed\n",
    "\n",
    "prd200.write.save(\"product2.json\", format=\"json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have two datasets in two views we can join them on the common column for queries. For example:\n",
    "    \n",
    "Get the list of customers and product categories in which they bought multiple items (quantity) that are more expensive than 200.00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "custlist200 = spark.sql(\"SELECT a.customer_name, b.product_category, count(*) as prdcount FROM customers a \\\n",
    "INNER JOIN products b ON a.customer_id=b.customer_id WHERE b.product_price>200.00 \\\n",
    "GROUP BY a.customer_name, b.product_category HAVING prdcount>1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(custlist200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+--------+\n",
      "|    customer_name|  product_category|prdcount|\n",
      "+-----------------+------------------+--------+\n",
      "|      David Smith|Hunting & Shooting|       2|\n",
      "|    William Weiss|Hunting & Shooting|       2|\n",
      "|    William Smith| Bike & Skate Shop|       2|\n",
      "|    William Smith|Hunting & Shooting|       3|\n",
      "|       Mary Smith| Bike & Skate Shop|      22|\n",
      "|   Kimberly Blair|Hunting & Shooting|       2|\n",
      "|     William Clay|Hunting & Shooting|       2|\n",
      "|   Margaret Smith|Hunting & Shooting|       2|\n",
      "|    Russell Smith|Hunting & Shooting|       2|\n",
      "|       Mary Lopez| Bike & Skate Shop|       2|\n",
      "|      Louis Novak|Hunting & Shooting|       2|\n",
      "|      Linda Smith| Bike & Skate Shop|       2|\n",
      "|       Mary Black| Bike & Skate Shop|       2|\n",
      "|       Linda Hale| Bike & Skate Shop|       2|\n",
      "|    Mary Gonzales|Hunting & Shooting|       2|\n",
      "|   Jesse Matthews|Hunting & Shooting|       2|\n",
      "|      Mary Daniel| Bike & Skate Shop|       2|\n",
      "|     Albert Smith|Hunting & Shooting|       2|\n",
      "|       Mary Sharp| Bike & Skate Shop|       2|\n",
      "|Christopher Smith|Hunting & Shooting|       2|\n",
      "+-----------------+------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "custlist200.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_name: string (nullable = true)\n",
      " |-- product_category: string (nullable = true)\n",
      " |-- prdcount: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "custlist200.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Syntax for reading a parquet file and loading as a DF\n",
    "\n",
    "productDF1 = spark.read.load(\"products.parquet\")\n",
    "\n",
    "We can use the above since parquet is the default format used by SparkSQL\n",
    "\n",
    "productDF2 = spark.read.load(\"products.parquet\", format=\"parquet\")\n",
    "\n",
    "productDF3 = spark.read.parquet(\"products.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "productDF1 = spark.read.load(\"product1.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+--------+\n",
      "|category_id|   product_category|prdcount|\n",
      "+-----------+-------------------+--------+\n",
      "|         40|        Accessories|       7|\n",
      "|         16|    As Seen on  TV!|       6|\n",
      "|          3|Baseball & Softball|       4|\n",
      "|         42|  Bike & Skate Shop|    1377|\n",
      "|         47|            Boating|       6|\n",
      "+-----------+-------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "productDF1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
