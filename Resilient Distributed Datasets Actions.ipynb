{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning PySpark \n",
    "### Video series\n",
    "\n",
    "### Packt Publishing\n",
    "\n",
    "**Author**: Tomasz Drabas\n",
    "**Date**:   2017-12-10\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Section 3: Resilient Distributed Datasets - Actions\n",
    "\n",
    "In this section we will look at the Resilient Distributed Datasets (RDDs) and the actions available.\n",
    "\n",
    "## Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderDate,Region,Rep,Item,Units,UnitCost,Total\r\n",
      "1/6/16,East,Jones,Pencil,95,1.99,189.05\r\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "head -n 2 ../data/sample_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>6</td><td>None</td><td>pyspark</td><td>idle</td><td></td><td></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "\n",
    "def parseCSVRow(inputRow):\n",
    "    try:\n",
    "        rowSplit = inputRow.split(',')\n",
    "        rowSplit[0] = dt.datetime.strptime(rowSplit[0], '%m/%d/%y')\n",
    "        rowSplit[4] = int(rowSplit[4])\n",
    "        \n",
    "        for i in [5,6]:\n",
    "            rowSplit[i] = float(rowSplit[i])\n",
    "        \n",
    "        return [rowSplit]\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "rdd_clean = sc.textFile('../data/sample_data.csv', 4) \\\n",
    "    .flatMap(parseCSVRow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .take(...) action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd_clean.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for element in rdd_clean.takeOrdered(5, key = lambda el: el[0]):\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for element in rdd_clean.takeSample(False, 5, seed=667):\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .collect(...) action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True"
     ]
    }
   ],
   "source": [
    "len(rdd_clean.collect()) == rdd_clean.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[datetime.datetime(2016, 7, 29, 0, 0), 'East', 'Parent', 'Binder', 81, 19.99, 1619.19]\n",
      "[datetime.datetime(2016, 12, 29, 0, 0), 'East', 'Parent', 'Pen Set', 74, 15.99, 1183.26]\n",
      "[datetime.datetime(2017, 2, 1, 0, 0), 'Central', 'Smith', 'Binder', 87, 15.0, 1305.0]\n",
      "[datetime.datetime(2017, 8, 7, 0, 0), 'Central', 'Kivell', 'Pen Set', 42, 23.95, 1005.9]\n",
      "[datetime.datetime(2017, 10, 14, 0, 0), 'West', 'Thompson', 'Binder', 57, 19.99, 1139.43]\n",
      "[datetime.datetime(2017, 12, 4, 0, 0), 'Central', 'Jardine', 'Binder', 94, 19.99, 1879.06]"
     ]
    }
   ],
   "source": [
    "for element in rdd_clean.filter(lambda el: el[-1] > 1000).collect():\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .reduce(...) action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18628.38"
     ]
    }
   ],
   "source": [
    "from operator import add\n",
    "\n",
    "total_value = rdd_clean \\\n",
    "    .map(lambda el: el[-1]) \\\n",
    "    .reduce(add)\n",
    "    \n",
    "total_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18628.38"
     ]
    }
   ],
   "source": [
    "total_value = rdd_clean \\\n",
    "    .map(lambda el: el[-1]) \\\n",
    "    .reduce(lambda x, y: x + y)\n",
    "\n",
    "total_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .reduceByKey(...) action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('East', 6002.090000000001)\n",
      "('Central', 10139.57)\n",
      "('West', 2486.7200000000003)"
     ]
    }
   ],
   "source": [
    "sales_by_region = rdd_clean \\\n",
    "    .map(lambda el: (el[1], el[-1])) \\\n",
    "    .reduceByKey(lambda x, y: x + y)\n",
    "    \n",
    "for element in sales_by_region.collect():\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .count() action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42"
     ]
    }
   ],
   "source": [
    "rdd_clean.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42"
     ]
    }
   ],
   "source": [
    "rdd_clean.countApprox(10, confidence=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11"
     ]
    }
   ],
   "source": [
    "sales = rdd_clean \\\n",
    "    .map(lambda el: el[2])\n",
    "\n",
    "sales.countApproxDistinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11"
     ]
    }
   ],
   "source": [
    "sales.distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .foreach(...) action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distinct_sales = sales.distinct()\n",
    "distinct_sales.foreach(print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .aggregate(...) action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18628.38, 42)"
     ]
    }
   ],
   "source": [
    "seqOp =  (lambda x, y: (x[0] + y,    x[1] + 1))\n",
    "combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "\n",
    "rdd_clean \\\n",
    "    .map(lambda el: el[-1]) \\\n",
    "    .aggregate((0.0,0), seqOp, combOp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .aggregateByKey(...) action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Jardine', 2812.19, 5, 562.438)\n",
      "('Gill', 1749.8700000000001, 5, 349.97400000000005)\n",
      "('Smith', 1641.43, 3, 547.1433333333333)\n",
      "('Howard', 536.75, 2, 268.375)\n",
      "('Thompson', 1203.1100000000001, 2, 601.5550000000001)\n",
      "('Jones', 2363.04, 8, 295.38)\n",
      "('Sorvino', 1283.6100000000001, 4, 320.90250000000003)\n",
      "('Andrews', 438.37, 4, 109.5925)\n",
      "('Morgan', 1387.77, 3, 462.59)\n",
      "('Parent', 3102.3, 3, 1034.1000000000001)\n",
      "('Kivell', 2109.94, 3, 703.3133333333334)"
     ]
    }
   ],
   "source": [
    "seqOp =  (lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "\n",
    "for element in rdd_clean \\\n",
    "    .map(lambda el: (el[2], (el[-1], 1))) \\\n",
    "    .aggregateByKey((0.0, 0), seqOp, combOp) \\\n",
    "    .map(lambda el: (el[0], el[1][0], el[1][1], el[1][0] / el[1][1])) \\\n",
    "    .collect():\n",
    "        print(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .coalesce(...) action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4"
     ]
    }
   ],
   "source": [
    "rdd_clean.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1"
     ]
    }
   ],
   "source": [
    "rdd_single = rdd_clean.coalesce(1)\n",
    "rdd_single.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .combineByKey(...) action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Jardine', [('Binder', 1933.95), ('Pencil', 449.1), ('Pen Set', 249.5)])\n",
      "('Gill', [('Pen', 539.73), ('Pencil', 77.4), ('Binder', 1132.74)])\n",
      "('Smith', [('Desk', 250.0), ('Pencil', 86.43), ('Binder', 1305.0)])\n",
      "('Howard', [('Pen', 479.04), ('Binder', 57.71)])\n",
      "('Thompson', [('Pencil', 63.68), ('Binder', 1139.43)])\n",
      "('Jones', [('Pen', 575.36), ('Binder', 559.36), ('Pencil', 363.70000000000005), ('Pen Set', 565.22)])\n",
      "('Sorvino', [('Pen', 151.24), ('Desk', 825.0), ('Pencil', 167.44), ('Binder', 139.93)])\n",
      "('Andrews', [('Pencil', 298.65000000000003), ('Binder', 139.72)])\n",
      "('Morgan', [('Pen Set', 686.95), ('Pencil', 449.1), ('Binder', 251.72)])\n",
      "('Parent', [('Binder', 1619.19), ('Pen', 299.85), ('Pen Set', 1183.26)])\n",
      "('Kivell', [('Desk', 625.0), ('Pen Set', 1484.94)])"
     ]
    }
   ],
   "source": [
    "def combiner(element):\n",
    "    return [element]\n",
    "\n",
    "def valueMerger(element1, element2):\n",
    "    element1.append(element2)\n",
    "    return element1\n",
    "\n",
    "def combinerMerger(element1, element2):\n",
    "    el1 = dict(element1)\n",
    "    \n",
    "    for e in element2:\n",
    "        if e[0] not in el1:\n",
    "            el1[e[0]] = 0\n",
    "\n",
    "        el1[e[0]] += e[1]\n",
    "    \n",
    "    return list(el1.items())\n",
    "\n",
    "for element in rdd_clean \\\n",
    "    .map(lambda el: (el[2], (el[3], el[-1]))) \\\n",
    "    .combineByKey(combiner, valueMerger, combinerMerger) \\\n",
    "    .collect():\n",
    "        print(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .histogram(...) action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9.0, 17)\n",
      "(196.0, 7)\n",
      "(383.0, 7)\n",
      "(570.0, 4)\n",
      "(757.0, 1)\n",
      "(944.0, 1)\n",
      "(1131.0, 3)\n",
      "(1318.0, 0)\n",
      "(1505.0, 1)\n",
      "(1692.0, 1)"
     ]
    }
   ],
   "source": [
    "hist = rdd_clean \\\n",
    "    .map(lambda el: el[-1]) \\\n",
    "    .histogram(10)\n",
    "\n",
    "for bucket in [(round(b, 0), v) for b, v in zip(hist[0], hist[1])]:\n",
    "    print(bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting data\n",
    "\n",
    "### sortBy(...) action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Jardine', '2017-12', 'Central', 1879.06)\n",
      "('Parent', '2016-07', 'East', 1619.19)\n",
      "('Smith', '2017-02', 'Central', 1305.0)\n",
      "('Parent', '2016-12', 'East', 1183.26)\n",
      "('Thompson', '2017-10', 'West', 1139.43)"
     ]
    }
   ],
   "source": [
    "for element in rdd_clean \\\n",
    "    .map(lambda el: (el[2], el[0].strftime('%Y-%m'), el[1], el[-1])) \\\n",
    "    .sortBy(lambda el: el[-1], ascending = False) \\\n",
    "    .take(5):\n",
    "        print(element)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sortByKey(...) action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1879.06, ('Jardine', '2017-12', 'Central'))\n",
      "(1619.19, ('Parent', '2016-07', 'East'))\n",
      "(1305.0, ('Smith', '2017-02', 'Central'))\n",
      "(1183.26, ('Parent', '2016-12', 'East'))\n",
      "(1139.43, ('Thompson', '2017-10', 'West'))"
     ]
    }
   ],
   "source": [
    "for element in rdd_clean \\\n",
    "    .map(lambda el: (el[-1], (el[2], el[0].strftime('%Y-%m'), el[1]))) \\\n",
    "    .sortByKey(ascending = False) \\\n",
    "    .take(5):\n",
    "        print(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving data\n",
    "\n",
    "### .saveAsTextFile(...) action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "rm -rf ../data/sample_data_cleaned.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd_clean.saveAsTextFile('../data/sample_data_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 8\n",
      "drwxr-xr-x   5 drabast  staff   160 Jan  9 22:30 .\n",
      "drwxr-xr-x  12 drabast  staff   384 Dec 15 00:00 ..\n",
      "-rw-r--r--@  1 drabast  staff  1927 Dec 15 21:56 sample_data.csv\n",
      "drwxr-xr-x  12 drabast  staff   384 Jan  9 22:30 sample_data_cleaned.csv\n",
      "drwxr-xr-x  12 drabast  staff   384 Jan  8 22:23 sample_data_cleaned.pkl\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "ls -la ../data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 72\n",
      "drwxr-xr-x  12 drabast  staff   384 Jan  9 22:30 .\n",
      "drwxr-xr-x   5 drabast  staff   160 Jan  9 22:30 ..\n",
      "-rw-r--r--   1 drabast  staff     8 Jan  9 22:30 ._SUCCESS.crc\n",
      "-rw-r--r--   1 drabast  staff    16 Jan  9 22:30 .part-00000.crc\n",
      "-rw-r--r--   1 drabast  staff    20 Jan  9 22:30 .part-00001.crc\n",
      "-rw-r--r--   1 drabast  staff    16 Jan  9 22:30 .part-00002.crc\n",
      "-rw-r--r--   1 drabast  staff    16 Jan  9 22:30 .part-00003.crc\n",
      "-rw-r--r--   1 drabast  staff     0 Jan  9 22:30 _SUCCESS\n",
      "-rw-r--r--   1 drabast  staff   767 Jan  9 22:30 part-00000\n",
      "-rw-r--r--   1 drabast  staff  1030 Jan  9 22:30 part-00001\n",
      "-rw-r--r--   1 drabast  staff   938 Jan  9 22:30 part-00002\n",
      "-rw-r--r--   1 drabast  staff   877 Jan  9 22:30 part-00003\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "ls -la ../data/sample_data_cleaned.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "rm -rf ../data/sample_data_cleaned_gzipped.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd_clean.saveAsTextFile(\n",
    "    '../data/sample_data_cleaned_gzipped.csv',\n",
    "    'org.apache.hadoop.io.compress.GzipCodec'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 72\n",
      "drwxr-xr-x  12 drabast  staff  384 Jan  9 22:32 .\n",
      "drwxr-xr-x   6 drabast  staff  192 Jan  9 22:32 ..\n",
      "-rw-r--r--   1 drabast  staff    8 Jan  9 22:32 ._SUCCESS.crc\n",
      "-rw-r--r--   1 drabast  staff   12 Jan  9 22:32 .part-00000.gz.crc\n",
      "-rw-r--r--   1 drabast  staff   12 Jan  9 22:32 .part-00001.gz.crc\n",
      "-rw-r--r--   1 drabast  staff   12 Jan  9 22:32 .part-00002.gz.crc\n",
      "-rw-r--r--   1 drabast  staff   12 Jan  9 22:32 .part-00003.gz.crc\n",
      "-rw-r--r--   1 drabast  staff    0 Jan  9 22:32 _SUCCESS\n",
      "-rw-r--r--   1 drabast  staff  266 Jan  9 22:32 part-00000.gz\n",
      "-rw-r--r--   1 drabast  staff  328 Jan  9 22:32 part-00001.gz\n",
      "-rw-r--r--   1 drabast  staff  324 Jan  9 22:32 part-00002.gz\n",
      "-rw-r--r--   1 drabast  staff  311 Jan  9 22:32 part-00003.gz\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "ls -la ../data/sample_data_cleaned_gzipped.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### .saveAsPickleFile(...) action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "rm -rf ../data/sample_data_cleaned.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd_clean.saveAsPickleFile('../data/sample_data_cleaned.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 72\n",
      "drwxr-xr-x  12 drabast  staff   384 Jan  9 22:33 .\n",
      "drwxr-xr-x   6 drabast  staff   192 Jan  9 22:33 ..\n",
      "-rw-r--r--   1 drabast  staff     8 Jan  9 22:33 ._SUCCESS.crc\n",
      "-rw-r--r--   1 drabast  staff    16 Jan  9 22:33 .part-00000.crc\n",
      "-rw-r--r--   1 drabast  staff    20 Jan  9 22:33 .part-00001.crc\n",
      "-rw-r--r--   1 drabast  staff    20 Jan  9 22:33 .part-00002.crc\n",
      "-rw-r--r--   1 drabast  staff    20 Jan  9 22:33 .part-00003.crc\n",
      "-rw-r--r--   1 drabast  staff     0 Jan  9 22:33 _SUCCESS\n",
      "-rw-r--r--   1 drabast  staff   951 Jan  9 22:33 part-00000\n",
      "-rw-r--r--   1 drabast  staff  1235 Jan  9 22:33 part-00001\n",
      "-rw-r--r--   1 drabast  staff  1157 Jan  9 22:33 part-00002\n",
      "-rw-r--r--   1 drabast  staff  1049 Jan  9 22:33 part-00003\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "ls -la ../data/sample_data_cleaned.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "values = rdd_clean \\\n",
    "    .map(lambda el: el[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### .mean() action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "443.53285714285715"
     ]
    }
   ],
   "source": [
    "values.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### .stdev() action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "438.90819278819419"
     ]
    }
   ],
   "source": [
    "values.stdev()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### .max() action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1879.06"
     ]
    }
   ],
   "source": [
    "values.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### .min() action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.03"
     ]
    }
   ],
   "source": [
    "values.min()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
