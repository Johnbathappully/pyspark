{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframe in PySpark: Overview\n",
    "\n",
    "In Apache Spark, a DataFrame is a distributed collection of rows under named columns. In simple terms, it is same as a table in relational database or an Excel sheet with Column headers. It also shares some common characteristics with RDD:\n",
    "- Immutable in nature : We can create DataFrame / RDD once but canâ€™t change it. And we can transform a DataFrame / RDD after applying transformations.\n",
    "- Lazy Evaluations: Which means that a task is not executed until an action is performed.\n",
    "- Distributed: RDD and DataFrame both are distributed in nature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why DataFrames are Useful?\n",
    "- DataFrames are designed for processing large collection of structured or semi-structured data.\n",
    "- Observations in Spark DataFrame are organised under named columns, which helps Apache Spark to understand the schema of a DataFrame. This helps Spark optimize execution plan on these queries.\n",
    "- DataFrame in Apache Spark has the ability to handle petabytes of data.\n",
    "- DataFrame has a support for wide range of data format and sources.\n",
    "- It has API support for different languages like Python, R, Scala, Java."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to create a DataFrame?\n",
    "\n",
    "A DataFrame in Apache Spark can be created in multiple ways:\n",
    "- It can be created using different data formats. For example, loading the data from JSON, CSV.\n",
    "- Loading data from Existing RDD.\n",
    "- Programmatically specifying schema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating DataFrame from RDD\n",
    "\n",
    "- Create a list of tuples. Each tuple contains name of a person with age.\n",
    "- Create a RDD from the list above.\n",
    "- Convert each tuple to a row.\n",
    "- Create a DataFrame by applying createDataFrame on RDD with the help of sqlContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Tools\\\\spark-3.3.0-bin-hadoop3'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Windows\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# importing spark \n",
    "import findspark\n",
    "findspark.init(\"/usr/local/spark/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName='Spark DataFrames')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Tools\\spark-3.3.0-bin-hadoop3\\python\\pyspark\\sql\\context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "lst = [('Ankit',25),('Jalfaizy',22),('saurabh',20),('Bala',26)]\n",
    "rdd = sc.parallelize(lst)\n",
    "people = rdd.map(lambda x: Row(name=x[0], age=int(x[1])))\n",
    "schemaPeople = sqlContext.createDataFrame(people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets check the type of schemaPeople.\n",
    "type(schemaPeople)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='Ankit', age=25),\n",
       " Row(name='Jalfaizy', age=22),\n",
       " Row(name='saurabh', age=20),\n",
       " Row(name='Bala', age=26)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schemaPeople.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+\n",
      "|    name|age|\n",
      "+--------+---+\n",
      "|   Ankit| 25|\n",
      "|Jalfaizy| 22|\n",
      "| saurabh| 20|\n",
      "|    Bala| 26|\n",
      "+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schemaPeople.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Departments\n",
    "dept1 = Row(id='123456', name='Computer Science')\n",
    "dept2 = Row(id='789012', name='Mechanical Engineering')\n",
    "dept3 = Row(id='345678', name='Theater and Drama')\n",
    "dept4 = Row(id='901234', name='Indoor Recreation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Employee = Row(\"firstName\", \"lastName\", \"email\", \"salary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp1 = Employee('ramesh', 'armbrust', 'no-reply@spds.edu', 100000)\n",
    "emp2 = Employee('suresh', 'meng', 'no-reply@spds.edu', 120000)\n",
    "emp3 = Employee('naresh', None, 'no-reply@spds.edu', 140000)\n",
    "emp4 = Employee(None, 'kamesh', 'no-reply@spds.edu', 160000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect Department with Employee\n",
    "deptWithEmp12 = Row(department=dept1, employees=[emp1, emp2])\n",
    "deptWithEmp34 = Row(department=dept2, employees=[emp3, emp4])\n",
    "deptWithEmp14 = Row(department=dept3, employees=[emp1, emp4])\n",
    "deptWithEmp23 = Row(department=dept4, employees=[emp2, emp3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = sqlContext.createDataFrame([deptWithEmp12,deptWithEmp34])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = sqlContext.createDataFrame([deptWithEmp14,deptWithEmp23])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unionDF = df1.union(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unionDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parquet\n",
    "\n",
    "Parquet is a columnar file format that provides optimizations to speed up queries and is a far more efficient file format than CSV or JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unionDF.write.parquet('df.parquet2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquetDF = sqlContext.read.parquet('df.parquet2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquetDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "rdd = sc.parallelize([Row(name='a', age=99, height=100), \\\n",
    "                      Row(name='b',age=50,height=100), \\\n",
    "                      Row(name='b',age=50,height=178)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = rdd.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
